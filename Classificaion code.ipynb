
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgbm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_dataset = pd.DataFrame(columns = ['accuracy_log', 'accuracy_log_rf_ensemble', 'accuracy_log_rf_gbm_ensemble', \n",
    "                                'accuracy_log_rf_gbm_xgb_ensemble', 'accuracy_log_rf_gbm_xgb_lgbm_ensemble', 'num_rows',\n",
    "                                'num_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "x = glob.glob(r'C:\\Users\\rohit\\Desktop\\MSc. Computer Science - Data Science\\Machine Learning\\Group project\\datasets\\classification\\*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(x)):\n",
    "    dataset = pd.read_csv(x[i])\n",
    "\n",
    "    #Dropping all NA rows\n",
    "    dataset= dataset.dropna()\n",
    "\n",
    "    # Labels are the values we want to predict\n",
    "    labels = np.array(dataset.iloc[:,-1])\n",
    "\n",
    "    # Remove the labels from the features, axis 1 refers to the columns\n",
    "    dataset= dataset.iloc[:, :-1]\n",
    "\n",
    "    #One hot encoding\n",
    "    dataset1 = pd.get_dummies(dataset)\n",
    "\n",
    "    # Saving feature names for later use\n",
    "    dataset1_list = list(dataset1.columns)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    dataset1 = np.array(dataset1)\n",
    "\n",
    "    # Using Skicit-learn to split data into training and testing sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_dataset1, test_dataset1, train_labels, test_labels = train_test_split(dataset1, labels, \n",
    "                                                                                test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    \n",
    "    #LogisticRegression\n",
    "    logistic_reg = LogisticRegression().fit(train_dataset1, train_labels)\n",
    "\n",
    "    pred_log = logistic_reg.predict(test_dataset1)\n",
    "\n",
    "    #Accuracy of Logistic regression model\n",
    "    accuracy_log = sklearn.metrics.accuracy_score(test_labels, pred_log, normalize=True, sample_weight=None)\n",
    "    \n",
    "    #RandomForest\n",
    "    rf_clf = RandomForestClassifier(n_estimators = 1000, random_state = 1).fit(train_dataset1, train_labels)\n",
    "\n",
    "    pred_rf = rf_clf.predict(test_dataset1)\n",
    "\n",
    "    #Accuracy of rf model\n",
    "    accuracy_rf = sklearn.metrics.accuracy_score(test_labels, pred_rf, normalize=True, sample_weight=None)\n",
    "    \n",
    "    \n",
    "    #Accuracy of ensemble model of Logistic and RandomForest\n",
    "    model = VotingClassifier(estimators=[('lr', logistic_reg), ('rf', rf_clf)], voting='hard')\n",
    "    model.fit(train_dataset1,train_labels)\n",
    "    accuracy_log_rf_ensemble = model.score(test_dataset1,test_labels)\n",
    "    \n",
    "\n",
    "    #Gradient Boosting\n",
    "    GBM_clf = GradientBoostingClassifier().fit(train_dataset1, train_labels)\n",
    "\n",
    "    pred_gbm = GBM_clf.predict(test_dataset1)\n",
    "\n",
    "    #Accuracy of gbm model\n",
    "    accuracy_gbm = sklearn.metrics.accuracy_score(test_labels, pred_gbm, normalize=True, sample_weight=None)\n",
    "    \n",
    "    \n",
    "    #Accuracy of ensemble model of Logistic , RandomForest and GBM\n",
    "    model = VotingClassifier(estimators=[('lr', logistic_reg), ('rf', rf_clf) , ('gbm', GBM_clf)], voting='hard')\n",
    "    model.fit(train_dataset1,train_labels)\n",
    "    accuracy_log_rf_gbm_ensemble = model.score(test_dataset1,test_labels)\n",
    "\n",
    "\n",
    "    #Xtreme Gradient Boosting\n",
    "    XGB_clf = XGBClassifier().fit(train_dataset1, train_labels)\n",
    "\n",
    "    pred_xgb = XGB_clf.predict(test_dataset1)\n",
    "\n",
    "    #Accuracy of xgb model\n",
    "    accuracy_xgb = sklearn.metrics.accuracy_score(test_labels, pred_xgb, normalize=True, sample_weight=None)\n",
    "    \n",
    "    \n",
    "    #Accuracy of ensemble model of Logistic , RandomForest , GBM and XGB\n",
    "    model = VotingClassifier(estimators=[('lr', logistic_reg), ('rf', rf_clf) , ('gbm', GBM_clf) , ('xgb', XGB_clf)], voting='hard')\n",
    "    model.fit(train_dataset1,train_labels)\n",
    "    accuracy_log_rf_gbm_xgb_ensemble = model.score(test_dataset1,test_labels)\n",
    "\n",
    "    \n",
    "    #Light Gradient Boosting\n",
    "    LGBM_clf = lgbm.LGBMClassifier().fit(train_dataset1, train_labels)\n",
    "\n",
    "    pred_lgbm = LGBM_clf.predict(test_dataset1)\n",
    "\n",
    "    #Accuracy of lgbm model\n",
    "    accuracy_lgbm = sklearn.metrics.accuracy_score(test_labels, pred_lgbm, normalize=True, sample_weight=None)\n",
    "\n",
    "    \n",
    "    #Accuracy of ensemble model of Logistic , RandomForest , GBM , XGB and LGBM\n",
    "    model = VotingClassifier(estimators=[('lr', logistic_reg), ('rf', rf_clf) , ('gbm', GBM_clf) , ('xgb', XGB_clf) , ('lgbm', LGBM_clf)], voting='hard')\n",
    "    model.fit(train_dataset1,train_labels)\n",
    "    accuracy_log_rf_gbm_xgb_lgbm_ensemble = model.score(test_dataset1,test_labels)    \n",
    "    \n",
    "    \n",
    "    ensemble_dataset = ensemble_dataset.append({'accuracy_logistic' :accuracy_log, \n",
    "                                            'accuracy_log_rf_ensemble': accuracy_log_rf_ensemble, \n",
    "                                            'accuracy_log_rf_gbm_ensemble':accuracy_log_rf_gbm_ensemble, \n",
    "                                            'accuracy_log_rf_gbm_xgb_ensemble':accuracy_log_rf_gbm_xgb_ensemble,\n",
    "                                            'accuracy_log_rf_gbm_xgb_lgbm_ensemble':accuracy_log_rf_gbm_xgb_lgbm_ensemble, \n",
    "                                            'num_rows': len(train_dataset1),\n",
    "                                            'num_cols': len((dataset1[0]))},\n",
    "                                             ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ensemble_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
